# SCC Vulnerability Ingestion Service

Automated service for ingesting vulnerability findings from GCP Security Command Center (SCC) with support for Firestore and Redis datastores.

## Features

- **Efficient Batch Processing**: Handles 60k+ vulnerability findings efficiently
- **Dual Datastore Support**: Choose between Firestore or Redis
- **Data Enrichment**: Automatically enriches vulnerabilities with appcode and LOB
- **Atomic Operations**: Ensures data consistency with upsert operations
- **Parallel Processing**: Configurable parallel workers for optimal performance
- **Cron Job Ready**: Designed to run as a scheduled Cloud Run job

## Architecture

```
┌─────────────────────────────────────────────────────────────┐
│                    SCC INGESTION FLOW                        │
└─────────────────────────────────────────────────────────────┘

1. Fetch Findings
   ┌──────────────┐
   │ Cloud        │
   │ Scheduler    │──────► Trigger every 6 hours
   └──────┬───────┘
          │
          ▼
   ┌──────────────┐
   │  SCC Client  │──────► Fetch vulnerability findings
   │              │        (60k+ findings)
   └──────┬───────┘
          │
          ▼
2. Load Mapping
   ┌──────────────┐
   │  Datastore   │──────► Load project mapping
   │  (Firestore/ │        {projectid: {appcode, lob}}
   │   Redis)     │
   └──────┬───────┘
          │
          ▼
3. Enrich Data
   ┌──────────────┐
   │  Ingestion   │──────► Enrich findings with
   │  Service     │        appcode & lob
   └──────┬───────┘
          │
          ▼
4. Batch Upsert
   ┌──────────────┐
   │  Datastore   │──────► Atomic batch upsert
   │  (500/batch) │        (Firestore: 500, Redis: 1000)
   └──────────────┘
```

## Files Created

```
scc-vulnerability-ingestion/
├── config.py                      # Configuration management
├── .env.dev                       # Development environment
├── firestore_datastore.py         # Firestore implementation
├── redis_datastore.py             # Redis implementation
├── datastore_factory.py           # Factory for datastore creation
├── scc_client.py                  # SCC API client
├── ingestion_service.py           # Main ingestion logic
├── main.py                        # Entry point
├── requirements.txt               # Python dependencies
├── Dockerfile                     # Container image
├── cloud_scheduler.yaml           # Scheduler configuration
├── deploy.sh                      # Deployment script
├── setup_project_mapping.py       # Helper for project mapping
├── project_mapping_sample.csv     # Sample data
├── README.md                      # Comprehensive documentation
└── QUICKSTART.md                  # Quick start guide
```

## Data Flow

### Input: SCC Findings
```json
{
  "finding_name": "organizations/123/sources/456/findings/789",
  "category": "VULNERABILITY",
  "severity": "HIGH",
  "resource_name": "//compute.googleapis.com/projects/my-project/...",
  "cve": "CVE-2023-1234",
  "cvss_score": 7.5
}
```

### Project Mapping (Firestore/Redis)
```json
{
  "projectid": "my-project-123",
  "appcode": "payment-service",
  "lob": "finance"
}
```

### Output: Enriched Vulnerability
```json
{
  "finding_id": "789",
  "finding_name": "organizations/123/sources/456/findings/789",
  "category": "VULNERABILITY",
  "state": "ACTIVE",
  "severity": "HIGH",
  "resource_name": "//compute.googleapis.com/projects/my-project/...",
  "resource_type": "google.compute.Instance",
  "project_id": "my-project-123",
  "appcode": "payment-service",
  "lob": "finance",
  "cve": "CVE-2023-1234",
  "cvss_score": 7.5,
  "create_time": "2025-10-30T12:00:00Z",
  "event_time": "2025-10-30T12:00:00Z",
  "last_seen": "2025-10-31T05:12:00Z"
}
```

## Configuration

### Environment Variables

**.env.dev:**
```bash
# GCP Configuration
GCP_PROJECT_ID=your-project-id
GCP_ORGANIZATION_ID=123456789012

# Datastore Configuration
DATASTORE_TYPE=firestore  # Options: firestore, redis

# Firestore Configuration
FIRESTORE_COLLECTION_VULNERABILITIES=scc-vulnerabilities-dev
FIRESTORE_COLLECTION_PROJECT_MAPPING=project-mapping-dev

# Redis Configuration (if using Redis)
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=
REDIS_DB=0
REDIS_KEY_PREFIX=scc:vuln:dev:
REDIS_KEY_TTL=86400

# Batch Processing
BATCH_SIZE=500
MAX_WORKERS=10

# SCC Configuration
SCC_FINDING_FILTER=category="VULNERABILITY"
SCC_PAGE_SIZE=1000

# Logging
LOG_LEVEL=INFO
```

## Datastore Options

### Option 1: Firestore

**Pros:**
- Native GCP integration
- Automatic scaling
- Strong consistency
- Rich querying capabilities
- No infrastructure management

**Cons:**
- Higher latency for reads
- Cost per operation
- 500 operations per batch limit

**Configuration:**
```bash
DATASTORE_TYPE=firestore
FIRESTORE_COLLECTION_VULNERABILITIES=scc-vulnerabilities
FIRESTORE_COLLECTION_PROJECT_MAPPING=project-mapping
```

**Atomicity:**
- Uses Firestore batch writes (max 500 operations)
- All operations in a batch succeed or fail together
- Uses finding_id as document ID for automatic upsert

### Option 2: Redis

**Pros:**
- Ultra-fast reads/writes
- Lower latency
- Higher throughput
- Cost-effective for large datasets
- TTL support for automatic cleanup

**Cons:**
- Requires Redis instance management
- Memory-based (cost scales with data size)
- Less rich querying

**Configuration:**
```bash
DATASTORE_TYPE=redis
REDIS_HOST=10.0.0.1
REDIS_PORT=6379
REDIS_KEY_PREFIX=scc:vuln:
REDIS_KEY_TTL=86400
```

**Atomicity:**
- Uses Redis pipelines for atomic batch operations
- All commands in pipeline execute atomically
- Uses finding_id as key for automatic upsert
- `SET` with `EX` for TTL support

## Performance Optimization

### Streaming Architecture

**Memory-Efficient Design:**
- Streams findings from SCC without loading all into memory
- Processes batches dynamically as they're fetched
- Suitable for Cloud Run with limited memory (512MB-1GB)
- Can handle 100k+ findings without memory issues

**How It Works:**
```
SCC API (async stream)
    ↓
Fetch Finding → Add to Batch
    ↓
Batch Full? → Schedule Processing Task
    ↓
Continue Fetching (non-blocking)
    ↓
Process Multiple Batches in Parallel
    ↓
Wait for Completion
```

**Benefits:**
- Constant memory usage regardless of dataset size
- Parallel processing while streaming
- No memory spikes from loading all findings
- Optimal for Cloud Run constraints

### Batch Processing

**Firestore:**
- Batch size: 500 (Firestore limit)
- Processes 60,000 findings in ~120 batches
- Estimated time: 2-5 minutes
- Memory usage: ~50-100MB (streaming)

### Parallel Processing

```python
# Sequential processing
await service.ingest_vulnerabilities()

# Parallel processing (recommended for 60k+ rows)
await service.ingest_vulnerabilities_parallel()
```

**Configuration:**
```bash
MAX_WORKERS=10  # Number of parallel workers
BATCH_SIZE=500  # Findings per batch
```

**Performance:**
- 60,000 findings with 10 workers
- Throughput: ~500-1000 records/second
- Total time: 60-120 seconds

## Setup Project Mapping

### Firestore

Create documents in `project-mapping` collection:

```python
from google.cloud import firestore

db = firestore.Client()
collection = db.collection('project-mapping-dev')

# Add project mappings
collection.document('project-1').set({
    'projectid': 'my-project-123',
    'appcode': 'payment-service',
    'lob': 'finance'
})

collection.document('project-2').set({
    'projectid': 'analytics-456',
    'appcode': 'analytics-platform',
    'lob': 'data'
})
```

### Redis

Use the helper method or Redis CLI:

```python
from redis_datastore import RedisDatastore

datastore = RedisDatastore()
await datastore.set_project_mapping('my-project-123', 'payment-service', 'finance')
await datastore.set_project_mapping('analytics-456', 'analytics-platform', 'data')
```

Or via Redis CLI:
```bash
redis-cli HSET scc:vuln:dev:project_mapping my-project-123 '{"appcode":"payment-service","lob":"finance"}'
```

## Deployment

### Local Testing

```bash
# Install dependencies
pip install -r requirements.txt

# Set environment
export ENVIRONMENT=dev

# Run ingestion
python main.py
```

### Cloud Run Deployment

**1. Build and push Docker image:**
```bash
gcloud builds submit --tag gcr.io/PROJECT_ID/scc-ingestion:latest
```

**2. Deploy to Cloud Run:**
```bash
gcloud run deploy scc-ingestion \
  --image gcr.io/PROJECT_ID/scc-ingestion:latest \
  --platform managed \
  --region asia-southeast1 \
  --memory 2Gi \
  --timeout 1800 \
  --set-env-vars ENVIRONMENT=prod \
  --service-account scc-ingestion@PROJECT_ID.iam.gserviceaccount.com \
  --no-allow-unauthenticated
```

**3. Create Cloud Scheduler job:**
```bash
gcloud scheduler jobs create http scc-vulnerability-ingestion \
  --schedule="0 */6 * * *" \
  --uri="https://scc-ingestion-SERVICE_URL/run" \
  --http-method=POST \
  --oidc-service-account-email=scc-ingestion@PROJECT_ID.iam.gserviceaccount.com \
  --location=asia-southeast1 \
  --time-zone="Asia/Singapore"
```

### IAM Permissions

**Service Account:** `scc-ingestion@PROJECT_ID.iam.gserviceaccount.com`

**Required Roles:**
```bash
# Security Command Center
roles/securitycenter.findingsViewer

# Firestore (if using Firestore)
roles/datastore.user

# Cloud Run
roles/run.invoker
```

**Grant permissions:**
```bash
# SCC permissions
gcloud organizations add-iam-policy-binding ORG_ID \
  --member="serviceAccount:scc-ingestion@PROJECT_ID.iam.gserviceaccount.com" \
  --role="roles/securitycenter.findingsViewer"

# Firestore permissions
gcloud projects add-iam-policy-binding PROJECT_ID \
  --member="serviceAccount:scc-ingestion@PROJECT_ID.iam.gserviceaccount.com" \
  --role="roles/datastore.user"
```

## Usage Examples

### Run Ingestion Job

```python
import asyncio
from datastore_factory import create_datastore
from ingestion_service import IngestionService

async def run_ingestion():
    datastore = create_datastore()
    service = IngestionService(datastore)
    
    # Run parallel ingestion
    stats = await service.ingest_vulnerabilities_parallel()
    
    print(f"Fetched: {stats['total_fetched']}")
    print(f"Upserted: {stats['total_upserted']}")
    
    await datastore.close()

asyncio.run(run_ingestion())
```

### Query Vulnerabilities

**Firestore:**
```python
from google.cloud import firestore

db = firestore.Client()

# Get all HIGH severity vulnerabilities
vulns = db.collection('scc-vulnerabilities') \
    .where('severity', '==', 'HIGH') \
    .where('state', '==', 'ACTIVE') \
    .stream()

for vuln in vulns:
    data = vuln.to_dict()
    print(f"{data['cve']}: {data['appcode']} - {data['lob']}")
```

**Redis:**
```python
import redis
import json

r = redis.Redis(host='localhost', port=6379, db=0)

# Get vulnerability by finding ID
key = 'scc:vuln:dev:finding-123'
data = r.get(key)
if data:
    vuln = json.loads(data)
    print(f"{vuln['cve']}: {vuln['appcode']} - {vuln['lob']}")
```

## Monitoring

### Logs

```bash
# View Cloud Run logs
gcloud logging read "resource.type=cloud_run_revision AND resource.labels.service_name=scc-ingestion" \
  --limit 50 \
  --format json

# View local logs
tail -f ingestion_*.log
```

### Metrics

**Key Metrics:**
- Total findings fetched
- Total vulnerabilities upserted
- Processing duration
- Throughput (records/second)
- Batch processing time

**Example Output:**
```
================================================================================
SCC Vulnerability Ingestion Job Completed
End Time: 2025-10-31T10:30:00
Duration: 95.23 seconds
Findings Fetched: 62,450
Vulnerabilities Upserted: 62,450
Projects Mapped: 1,234
Batches Processed: 125
Throughput: 655.78 records/second
Datastore Stats: {'datastore_type': 'firestore', 'total_vulnerabilities': 62450}
================================================================================
```

## Troubleshooting

### Issue: Slow Performance

**Solution:**
- Increase `MAX_WORKERS` (default: 10)
- Increase `BATCH_SIZE` (Firestore: max 500, Redis: 1000+)
- Use Redis for faster writes
- Use parallel ingestion mode

### Issue: Memory Errors

**Solution:**
- Decrease `BATCH_SIZE`
- Decrease `MAX_WORKERS`
- Increase Cloud Run memory allocation
- Use sequential ingestion mode

### Issue: Missing Appcode/LOB

**Solution:**
- Verify project mapping exists in datastore
- Check project ID extraction from resource name
- Review logs for mapping warnings

### Issue: Duplicate Data

**Solution:**
- Upsert operations prevent duplicates
- Firestore: Uses `finding_id` as document ID
- Redis: Uses `finding_id` in key name
- Both ensure atomicity

## Cost Optimization

### Firestore
- Use TTL policies for old data
- Index only necessary fields
- Monitor read/write operations

### Redis
- Set appropriate TTL (default: 24 hours)
- Use Redis Memorystore (managed)
- Monitor memory usage

### Cloud Run
- Set appropriate memory limits
- Use minimum CPU allocation
- Schedule during off-peak hours

## Best Practices

1. **Run during off-peak hours** (e.g., every 6 hours)
2. **Monitor ingestion duration** and adjust batch size
3. **Set up alerting** for failed ingestion jobs
4. **Regularly review** project mappings
5. **Archive old vulnerabilities** to reduce datastore size
6. **Use Redis for high-throughput** scenarios
7. **Use Firestore for rich querying** requirements

## License

Internal use only.
