# Quick Start Guide

## Prerequisites

1. GCP Project with billing enabled
2. Security Command Center API enabled
3. Firestore or Redis instance
4. Python 3.11+

## Step 1: Install Dependencies

```bash
pip install -r requirements.txt
```

## Step 2: Configure Environment

Edit `.env.dev`:

```bash
# GCP Configuration
GCP_PROJECT_ID=your-project-id
GCP_ORGANIZATION_ID=123456789012

# Choose datastore
DATASTORE_TYPE=firestore  # or redis

# Firestore settings
FIRESTORE_COLLECTION_VULNERABILITIES=scc-vulnerabilities-dev
FIRESTORE_COLLECTION_PROJECT_MAPPING=project-mapping-dev
```

## Step 3: Setup Project Mapping

### Option A: Load Sample Data

```bash
export ENVIRONMENT=dev
python setup_project_mapping.py sample
```

### Option B: Load from CSV

```bash
python setup_project_mapping.py csv project_mapping_sample.csv
```

### Verify Mapping

```bash
python setup_project_mapping.py verify
```

## Step 4: Run Ingestion (Local Test)

```bash
export ENVIRONMENT=dev
python main.py
```

Expected output:
```
================================================================================
SCC Vulnerability Ingestion Job Started
Start Time: 2025-10-31T10:00:00
Datastore: firestore
================================================================================
Fetched 1000 findings from SCC...
Fetched 2000 findings from SCC...
...
Progress: Fetched 60000, Upserted 60000
================================================================================
SCC Vulnerability Ingestion Job Completed
Duration: 95.23 seconds
Findings Fetched: 62,450
Vulnerabilities Upserted: 62,450
Throughput: 655.78 records/second
================================================================================
```

## Step 5: Deploy to Cloud Run

### Set environment variables:

```bash
export GCP_PROJECT_ID=your-project-id
export GCP_ORGANIZATION_ID=123456789012
export REGION=asia-southeast1
export ENVIRONMENT=prod
```

### Run deployment script:

```bash
chmod +x deploy.sh
./deploy.sh
```

### Manually trigger the job:

```bash
gcloud scheduler jobs run scc-ingestion \
  --location=asia-southeast1 \
  --project=your-project-id
```

## Step 6: Monitor Execution

### View logs:

```bash
gcloud logging read \
  "resource.type=cloud_run_revision AND resource.labels.service_name=scc-ingestion" \
  --limit 50 \
  --project=your-project-id
```

### Check Cloud Scheduler:

```bash
gcloud scheduler jobs describe scc-ingestion \
  --location=asia-southeast1 \
  --project=your-project-id
```

## Datastore Comparison

| Feature | Firestore | Redis |
|---------|-----------|-------|
| **Setup** | No infrastructure | Requires Redis instance |
| **Performance** | Good (2-5 min for 60k) | Excellent (1-3 min for 60k) |
| **Cost** | Pay per operation | Pay for memory |
| **Querying** | Rich queries | Basic key-value |
| **TTL** | Manual cleanup | Automatic (configurable) |
| **Atomicity** | Batch writes (500) | Pipeline (1000+) |
| **Best For** | Rich querying needs | High throughput |

## Performance Tuning

### For 60k+ Vulnerabilities:

**Firestore:**
```bash
BATCH_SIZE=500      # Max for Firestore
MAX_WORKERS=10      # Parallel workers
```

**Redis:**
```bash
BATCH_SIZE=1000     # Higher throughput
MAX_WORKERS=15      # More parallelism
REDIS_KEY_TTL=86400 # 24 hours
```

### Cloud Run Resources:

```bash
--memory 2Gi        # For 60k+ records
--cpu 2             # Parallel processing
--timeout 1800      # 30 minutes max
```

## Troubleshooting

### Issue: "Permission denied"

**Solution:**
```bash
# Grant SCC permissions
gcloud organizations add-iam-policy-binding ORG_ID \
  --member="serviceAccount:scc-ingestion@PROJECT_ID.iam.gserviceaccount.com" \
  --role="roles/securitycenter.findingsViewer"
```

### Issue: "No findings returned"

**Solution:**
- Verify SCC is enabled
- Check organization ID is correct
- Verify findings exist in SCC console

### Issue: "Slow performance"

**Solution:**
- Switch to Redis for faster writes
- Increase MAX_WORKERS
- Use parallel ingestion mode

### Issue: "Memory errors"

**Solution:**
- Decrease BATCH_SIZE
- Increase Cloud Run memory
- Use sequential ingestion mode

## Next Steps

1. **Schedule regular ingestion** (every 6 hours recommended)
2. **Set up monitoring** and alerting
3. **Create dashboards** for vulnerability metrics
4. **Implement data retention** policies
5. **Add custom enrichment** logic as needed

## Support

For issues or questions, contact the platform team.
