"""Service for ingesting and enriching vulnerability data"""

import logging
from typing import List, Dict, Any
import asyncio
from datetime import datetime
from firestore_datastore import Datastore
from scc_client import SCCClient
import config

logger = logging.getLogger(__name__)


class IngestionService:
    """Service for ingesting vulnerability data from SCC"""
    
    def __init__(self, datastore: Datastore):
        self.datastore = datastore
        self.scc_client = SCCClient()
        self.project_mapping = {}
        logger.info("Initialized IngestionService")
    
    async def load_project_mapping(self):
        """Load project mapping from datastore"""
        logger.info("Loading project mapping...")
        self.project_mapping = await self.datastore.get_project_mapping()
        logger.info(f"Loaded {len(self.project_mapping)} project mappings")
    
    def enrich_vulnerability(self, vulnerability: Dict[str, Any]) -> Dict[str, Any]:
        """
        Enrich vulnerability with appcode, lob, and last_seen timestamp.
        
        Args:
            vulnerability: Vulnerability data
            
        Returns:
            Enriched vulnerability data
        """
        project_id = vulnerability.get('project_id')
        
        # Add appcode and lob from project mapping
        if project_id and project_id in self.project_mapping:
            mapping = self.project_mapping[project_id]
            vulnerability['appcode'] = mapping.get('appcode', 'unknown')
            vulnerability['lob'] = mapping.get('lob', 'unknown')
        else:
            vulnerability['appcode'] = 'unknown'
            vulnerability['lob'] = 'unknown'
            if project_id:
                logger.debug(f"No mapping found for project: {project_id}")
        
        # Add last_seen timestamp (ingestion time)
        vulnerability['last_seen'] = datetime.utcnow().isoformat()
        
        return vulnerability
    
    async def ingest_vulnerabilities(self) -> Dict[str, Any]:
        """
        Main ingestion process:
        1. Load project mapping
        2. Fetch findings from SCC asynchronously
        3. Enrich with appcode/lob
        4. Batch upsert to datastore
        
        Returns:
            Statistics dictionary
        """
        logger.info("Starting vulnerability ingestion...")
        
        # Load project mapping first
        await self.load_project_mapping()
        
        # Process in batches
        batch = []
        total_fetched = 0
        total_upserted = 0
        
        # Fetch findings from SCC asynchronously
        async for finding in self.scc_client.fetch_vulnerability_findings():
            # Enrich finding
            enriched_finding = self.enrich_vulnerability(finding)
            batch.append(enriched_finding)
            total_fetched += 1
            
            # When batch is full, upsert to datastore
            if len(batch) >= config.BATCH_SIZE:
                upserted = await self.datastore.upsert_vulnerabilities(batch)
                total_upserted += upserted
                logger.info(f"Progress: Fetched {total_fetched}, Upserted {total_upserted}")
                batch = []
        
        # Upsert remaining batch
        if batch:
            upserted = await self.datastore.upsert_vulnerabilities(batch)
            total_upserted += upserted
        
        logger.info(f"Ingestion complete: Fetched {total_fetched}, Upserted {total_upserted}")
        
        return {
            'total_fetched': total_fetched,
            'total_upserted': total_upserted,
            'projects_mapped': len(self.project_mapping)
        }
    
    async def ingest_vulnerabilities_parallel(self) -> Dict[str, Any]:
        """
        Streaming parallel ingestion process.
        Fetches from SCC asynchronously and processes batches dynamically
        without loading all findings into memory.
        
        Returns:
            Statistics dictionary
        """
        logger.info("Starting streaming parallel vulnerability ingestion...")
        
        # Load project mapping first
        await self.load_project_mapping()
        
        # Counters
        total_fetched = 0
        total_upserted = 0
        batches_processed = 0
        
        # Semaphore to limit concurrent batch processing
        semaphore = asyncio.Semaphore(config.MAX_WORKERS)
        
        # Queue to hold pending tasks
        pending_tasks = []
        
        # Current batch being accumulated
        current_batch = []
        
        async def process_batch(batch_findings: List[Dict[str, Any]]) -> int:
            """Process a single batch with semaphore"""
            async with semaphore:
                enriched = [self.enrich_vulnerability(f) for f in batch_findings]
                return await self.datastore.upsert_vulnerabilities(enriched)
        
        # Stream findings and process in batches
        logger.info("Streaming findings from SCC...")
        async for finding in self.scc_client.fetch_vulnerability_findings():
            current_batch.append(finding)
            total_fetched += 1
            
            # When batch is full, schedule it for processing
            if len(current_batch) >= config.BATCH_SIZE:
                # Create task for this batch
                task = asyncio.create_task(process_batch(current_batch))
                pending_tasks.append(task)
                batches_processed += 1
                
                # Log progress
                if batches_processed % 10 == 0:
                    logger.info(f"Progress: Fetched {total_fetched}, Scheduled {batches_processed} batches")
                
                # Reset batch
                current_batch = []
                
                # If we have too many pending tasks, wait for some to complete
                if len(pending_tasks) >= config.MAX_WORKERS * 2:
                    # Wait for at least half to complete
                    done, pending_tasks = await asyncio.wait(
                        pending_tasks,
                        return_when=asyncio.FIRST_COMPLETED
                    )
                    pending_tasks = list(pending_tasks)
                    
                    # Collect results from completed tasks
                    for task in done:
                        total_upserted += await task
        
        # Process remaining batch
        if current_batch:
            task = asyncio.create_task(process_batch(current_batch))
            pending_tasks.append(task)
            batches_processed += 1
        
        # Wait for all remaining tasks to complete
        if pending_tasks:
            logger.info(f"Waiting for {len(pending_tasks)} remaining batches to complete...")
            results = await asyncio.gather(*pending_tasks)
            total_upserted += sum(results)
        
        logger.info(f"Streaming parallel ingestion complete: Fetched {total_fetched}, Upserted {total_upserted}")
        
        return {
            'total_fetched': total_fetched,
            'total_upserted': total_upserted,
            'projects_mapped': len(self.project_mapping),
            'batches_processed': batches_processed
        }
